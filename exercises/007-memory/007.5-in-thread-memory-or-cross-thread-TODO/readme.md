The theory here is based on this [browseruse](https://mem0.ai/blog/how-browseruse-achieved-98-task-completion-and-41-cost-reduction-with-mem0/) use case.

For extremely long-running tasks, you may want to keep in thread memory to keep the LLM focused on what it's been doing. This might end up being more efficient, certainly token-wise, than retaining the entire conversation in memory.

Just an interesting discussion more than anything.
